# Imports
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, mean_absolute_error, r2_score
import random
from scipy.stats import spearmanr


# Pitching dataset without team stats
df = pd.read_csv('PitchingDraftTable_with_teamstats.csv', delimiter=',')


# list and drop columns that are less related to the target
cols_to_drop = ['name', 'playerid', 'mlbamid', 'team', 'Acronym', 'Full Name', 'Full Team Name', 'Drafted By', 'Drafted From', 'team_new', 'division', 'team_id', 'team_teamstats', 'Difference']
df = df.drop(columns=cols_to_drop)
print(df.columns.tolist())

# convert the target to numerical values
df['Drafted?'] = df['Drafted?'].astype(int)


df.head()


df.info()


player_features = list(df[['age', 'w', 'l', 'era', 'g', 'gs', 'cg', 'sho', 'sv', 'ip', 'tbf',
                    'h', 'r', 'er', 'hr', 'bb', 'hbp', 'wp', 'bk', 'so', 'k/9', 'bb/9', 'k/bb',
                    'hr/9', 'k%', 'bb%', 'k-bb%', 'avg', 'whip', 'babip', 'lob%', 'fip', 'e-f']].columns)

team_features = list(df[['W', 'L', 'T', 'G', 'WPCT', 'PE', 'BB (Batting)', 'AB', 'H', 'BA', 'DP', 
                    'DPPG', '2B', '2BPG', 'IP', 'R (Pitching)', 'ER', 'ERA', 'PO', 'A', 'E', 
                    'FPCT', 'HB', 'HBP', 'HA', 'HAPG', 'HR', 'HRPG', 'SF', 'SH', 'OBP', 
                    'SB', 'SBPG', 'CS', 'R (Batting)', 'RPG', 'SHO', 'TB', 'SLG', 'SO', 'BB (Pitching)', 
                    'K/BB', 'K/9', 'TP', '3B', '3BPG', 'WHIP', 'BBPG (Pitching)']].columns)

redundant_features = ['k%', 'bb%', 'K/BB', 'k/bb']
lasso_selected_features = [
    'age', 'fip', 'K/9', 'WHIP', 'so', 'g', 'L', 'hr/9', 'wp', 'l', 'ER',
    'HRPG', 'SF', 'babip', 'SHO', 'A', 'bk', 'BB (Pitching)', 'PE', '3B',
    'ip', 'FPCT', 'CS', 'w', 'HB', 'DPPG', 'SH', 'T', 'SO', 'BBPG (Pitching)',
    'SBPG', 'cg', 'RPG', 'k-bb%', 'HBP', '2BPG', 'sho', 'e-f', 'lob%', 'sv', 'E'
]
lasso_selected_features = [f for f in lasso_selected_features if f not in redundant_features]
features = player_features + team_features
#features = lasso_selected_features

df = pd.get_dummies(df, columns=['league'], drop_first=True)
df = pd.get_dummies(df, columns=['team_old'], drop_first=True)

X = df[features]
y = df['Drafted?'].astype(int)


# Separate the two classes
drafted_indices = df[df['Drafted?'] == 1].index
not_drafted_indices = df[df['Drafted?'] == 0].index

print(f"Original class distribution:")
print(f"  Class 0 (Not Drafted): {len(not_drafted_indices)}")
print(f"  Class 1 (Drafted): {len(drafted_indices)}")

# Randomly sample from class 0 to match class 1 size
np.random.seed(42)  # for reproducibility
undersampled_not_drafted = np.random.choice(not_drafted_indices, size=len(drafted_indices), replace=False)

# Combine the balanced indices
balanced_indices = np.concatenate([drafted_indices, undersampled_not_drafted])
np.random.shuffle(balanced_indices)

# Create balanced dataset
X_balanced = X.loc[balanced_indices]
y_balanced = y.loc[balanced_indices]

print(f"\nBalanced class distribution:")
print(f"  Class 0 (Not Drafted): {sum(y_balanced == 0)}")
print(f"  Class 1 (Drafted): {sum(y_balanced == 1)}")
print(f"  Total samples: {len(y_balanced)}")
print(f"  Class 1 percentage: {sum(y_balanced == 1) / len(y_balanced) * 100:.1f}%")


X_train, X_test, y_train, y_test = train_test_split(
    X_balanced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced
)


neg_count = sum(y_train == 0)
pos_count = sum(y_train == 1)
scale_pos_weight = neg_count / pos_count

print(f"\nTraining set distribution:")
print(f"  Class 0: {neg_count}")
print(f"  Class 1: {pos_count}")
print(f"  Scale_pos_weight: {scale_pos_weight:.2f}")


draft_model = xgb.XGBClassifier(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    scale_pos_weight=scale_pos_weight,
    random_state=42
)

draft_model.fit(X_train, y_train)

y_pred = draft_model.predict(X_test)
y_proba = draft_model.predict_proba(X_test)[:,1]

print("\nDrafted? Classifier Results (Balanced Dataset):")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.3f}\n")
print(classification_report(y_test, y_pred, digits=3))


importance = draft_model.get_booster().get_score(importance_type='gain')
importance_df = pd.DataFrame({
    'Feature': list(importance.keys()),
    'Importance': list(importance.values())
}).sort_values(by='Importance', ascending=False)

print("\nTop 20 Features by Importance:")
print(importance_df.head(20))


target_drafted = 'Drafted?'
target_pick = 'Pick'

drafted_df = df[df[target_drafted] == 1]
X_drafted = drafted_df[features]
y_pick = drafted_df[target_pick]

X_train_pick, X_test_pick, y_train_pick, y_test_pick = train_test_split(
    X_drafted, y_pick, test_size=0.2, random_state=42
)

pick_model = xgb.XGBRegressor(
    n_estimators=1000,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.7,
    random_state=42
)

pick_model.fit(X_train_pick, y_train_pick)

pick_preds = pick_model.predict(X_test_pick)

print("\nPick Prediction:")
print(f"MAE: {mean_absolute_error(y_test_pick, pick_preds):.3f}")

corr, _ = spearmanr(y_test_pick, pick_preds)
print(f"Spearman correlation: {corr:.3f}")


plt.scatter(y_test_pick, pick_preds, alpha=0.6)
plt.xlabel("Actual Pick")
plt.ylabel("Predicted Pick")
plt.title("Actual vs Predicted Draft Picks")
plt.show()

import matplotlib.pyplot as plt
plt.hist(y_pick, bins=30)
plt.xlabel("Pick Number")
plt.ylabel("Count")
plt.title("Draft Pick Distribution")
plt.show()


from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

target_drafted = 'Drafted?'
X = df[features]
y = df[target_drafted]
X_clean = X.dropna()
y_clean = y.loc[X_clean.index]

log_reg = make_pipeline(
    StandardScaler(),
    LogisticRegression(max_iter=5000, solver='liblinear')
)
log_reg.fit(X_clean, y_clean)



coefficients = log_reg.named_steps['logisticregression'].coef_[0]
coef_df = pd.DataFrame({
    'Feature': features,
    'LogReg_Weight': coefficients
}).sort_values(by='LogReg_Weight', ascending=False)

importance = draft_model.get_booster().get_score(importance_type='gain')
importance_df = pd.DataFrame({
    'Feature': list(importance.keys()),
    'XGB_Importance': list(importance.values())
})

# Merge the two
comparison_df = coef_df.merge(importance_df, on='Feature', how='inner')

# Sort by XGBoost importance or weight magnitude
comparison_df = comparison_df.sort_values(by='XGB_Importance', ascending=False)
comparison_df = comparison_df.reset_index(drop=True)
pd.set_option('display.max_rows', None)
print(comparison_df)

#print(comparison_df.head(20))



missing_summary = X.isnull().mean().sort_values(ascending=False)
print(missing_summary[missing_summary > 0].head(1000))
